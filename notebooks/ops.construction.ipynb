{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN construction from scratch\n",
    "\n",
    "Walkthrough of the functionality in `rnnops.ops.construction`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pseudoinverse rule for constructing RNNs with specific fixed points \n",
    "\n",
    "Suppose we are interested in constructing an $N$-neuron recurrent network which has specific fixed points under various input conditions.\n",
    "\n",
    "The dynamics of our network, with rate vector $x$, connectivity matrix $J$, element-wise nonlinearity $\\phi$, and inputs $u$, are given by\n",
    "$$\n",
    "\\frac{dx}{dt} = f(x, u) = -x +J \\phi(x) + u.\n",
    "$$\n",
    "In general, $x^\\mu$ is a fixed point under input $u^\\mu$ if:\n",
    "$$\n",
    "0 =  f(x^\\mu, u^\\mu) = -x^\\mu +J \\phi(x^\\mu) + u^\\mu,\n",
    "$$\n",
    "i.e.,\n",
    "$$\n",
    "J \\phi(x^\\mu) = x^\\mu - u^\\mu, \\\\\n",
    "J a^\\mu = b^\\mu.\n",
    "$$\n",
    "\n",
    "In the last line we made the assignments $a^\\mu = \\phi(x^\\mu)$ and $b^\\mu = x^\\mu - u^\\mu$. By the so-called pseudoinverse learning rule [1], we may make $a^\\mu$s and $b^\\mu$s into the columns of two matrices $A$ and $B$, and solve the least squares problem\n",
    "$$\n",
    "(*) \\quad J A = B\n",
    "$$\n",
    "to obtain a connectivity matrix $\\hat J$ which enforces these fixed points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "7.961633521956537\n"
     ]
    }
   ],
   "source": [
    "from rnnops.ops.construction import pseudoinverse_rule\n",
    "import numpy as np\n",
    "n_rec = 100\n",
    "n_fixed = 10\n",
    "x_fixed = np.random.randn(n_rec, n_fixed)\n",
    "u_fixed = np.random.randn(n_rec, n_fixed)\n",
    "J = pseudoinverse_rule(x_fixed, u_fixed, nonlinearity='relu')\n",
    "print(J.shape)\n",
    "print(np.linalg.norm(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the pseudoinverse learning rule to construct RNNs that compute Boolean functions\n",
    "\n",
    "Suppose a network with $N$ recurrent neurons is to compute the boolean function $y = F(z)$, where $y$ and $z$ are Boolean-valued vectors. Under input\n",
    "$$\n",
    "u^\\mu = W_{in}^{(1)} z^\\mu \\in R^N,\n",
    "$$\n",
    "we want to construct a fixed point $x^\\mu$ such that $y^\\mu = W^{out} x^{\\mu}$. Assume there is negligible overlap between all of the input and output weights (i.e. they are mutually orthogonal).\n",
    "\n",
    "How to choose the $x^\\mu$s? As stated initially, we want to enforce\n",
    "$$\n",
    "(**)\\quad  W_{out} B = Y,\n",
    "$$\n",
    "where the columns of $Y$ are the $y^\\mu$s. So, $B$ itself is the solution to a squares problem, of which one solution is\n",
    "\n",
    "$$\n",
    "x^\\mu = W_{out}^+ y^\\mu + u^\\mu.\n",
    "$$\n",
    "\n",
    "Then the optimal value $\\hat J$ is the least-norm solution to the equation $(**)$, which can be given closed form via the pseudoinverse. Note that the rank of $\\hat J$ is therefore upper bounded by both the number of readout vectors and the number of fixed points, as well as the rank of the matrix of output values $Y$.\n",
    "\n",
    "Also note that any $J$ constructed to satisfy $(*)$ and $(**)$ will work. More succinctly, $J$ must satisfy\n",
    "$$\n",
    "W_{out} J A = Y \n",
    "$$\n",
    "\n",
    "To recap, the recipe we use here is:\n",
    "\n",
    "1) As an optional preparatory step, expand the input dimensionality such that each dimension of the original input is paired with a dimension that is one minus the original value. This is equivalent to inserting a linear layer (with biases) between the inputs and the recurrent layer.\n",
    "\n",
    "2) Given $W_{out}$, $W_{in}$ and the input-output pairs $(z^\\mu ,y^\\mu )$, let $u^\\mu = W_{in} z^\\mu$ and solve $(**)$ for $b^\\mu$ and let $x^\\mu = b^\\mu + u^\\mu$. \n",
    "\n",
    "3) Let $a^\\mu = \\phi(x^\\mu)$ and solve $(*)$ for $J$ via the pseudoinverse rule.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR conditions:\n",
      "((0, 0), (0,))\n",
      "((0, 1), (1,))\n",
      "((1, 0), (1,))\n",
      "((1, 1), (0,))\n",
      "\n",
      "XOR conditions with expanded inputs:\n",
      "((0, 1, 0, 1), (0,))\n",
      "((0, 1, 1, 0), (1,))\n",
      "((1, 0, 0, 1), (1,))\n",
      "((1, 0, 1, 0), (0,))\n"
     ]
    }
   ],
   "source": [
    "# look at the inputs and outputs of the boolean XOR function\n",
    "from rnnops.ops.construction import XOR_conditions, expand_condition_inputs\n",
    "print('XOR conditions:')\n",
    "print('\\n'.join([str(_) for _ in XOR_conditions]))\n",
    "\n",
    "\n",
    "print('\\nXOR conditions with expanded inputs:')\n",
    "expanded_XOR_conditions = expand_condition_inputs(XOR_conditions)\n",
    "print('\\n'.join([str(_) for _ in expanded_XOR_conditions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN object \n",
      " signature: (4, 1)\n",
      " n_rec: 600\n",
      " nonlinearity: relu\n"
     ]
    }
   ],
   "source": [
    "from rnnops.ops.construction import construct_boolean_integration_rnn\n",
    "construction_args = {\n",
    "    'conditions': XOR_conditions,\n",
    "    'n_rec': 600,\n",
    "    'nonlinearity': 'relu',\n",
    "    'expand_inputs': True,\n",
    "}\n",
    "rnn = construct_boolean_integration_rnn(**construction_args)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### References\n",
    "\n",
    "[1] Personnaz, L., Guyon, I., & Dreyfus, G. (1986). Collective computational properties of neural networks: New learning mechanisms. *Physical Review A*, 34(5), 4217."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
